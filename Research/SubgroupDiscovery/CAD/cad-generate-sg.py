# -*- coding: utf-8 -*-
"""
Created on Fri Nov 17 13:54:34 2017

@author: hemlo
"""

import os
import pandas as pd
import numpy as np
import time
import datetime
from sklearn.model_selection import StratifiedKFold
from sklearn import linear_model
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
from scipy import stats
from sklearn.metrics import roc_curve, roc_auc_score

## Import Orange data mining package
import Orange

## Import necessary r objects and packages.
import rpy2.robjects as ro
from rpy2.robjects.packages import importr

base = importr('base')
utils = importr('utils')
Ckmeans = importr('Ckmeans.1d.dp')

## Create Ckmeans function and make it accessible to our code.  
ro.r('''
     #Ckmeans clustering function
     #returns Ckmeans cluster with appropriate k value
     ck <- function(r, k=NULL, verbose=FALSE) {
        Ckmeans.1d.dp(r,k)

        }
     
     #Fisher test for checking statistical significance of hidden feature
     #Returns test statistic and p-value
     
     fisher <-function(data){
       fisher.test(data, alternative='two.sided')
     }
     ''')
ck = ro.globalenv['ck']
fisher = ro.globalenv['fisher']
os.chdir('D:\\TTU\\NEL_Research\\SD Work\\CAD\\SDToCkmeans')
data = pd.read_csv('..\\UQ_rf_results.csv')

# Currently using Random Forest as classifier
clf = RandomForestClassifier(n_jobs=-1)

y = data.Class.ravel()
X = data.drop(['Class', 'Stability'], axis=1)
dropped_feature = pd.read_csv('..\\dropped_feature.csv')

columns = ['SG', 'Rule', 'SG_size', 'count_Class_0', 'count_Class_1', 'Accuracy', 'AUROC', 'len_hf1', 'len_hf2', 'len_nothf', 'num_hf1_true', 'num_hf2_true', 'num_nothf_true', 'hf1_pvalue_hf2',  'hf1_pvalue_nothf']
results = pd.DataFrame(columns = columns)

learner = Orange.classification.CN2SDUnorderedLearner()

learner.rule_finder.search_algorithm.beam_width = 5
learner.weighted_cover_and_remove = 0.7
learner.rule_finder.general_validator.max_rule_length= 5

classifier = learner(Orange.data.Table('sdtock-cad-orange-format.csv'))
X['Cluster_feature'] = 0

current_predictions = np.empty(y.shape)
current_probabilities = np.empty(y.shape)

clf = RandomForestClassifier(n_jobs=-1)
kf = StratifiedKFold(n_splits=10, shuffle=True)
plt.figure("sdtock", figsize=(7,7))
for i in range(0,3):
    # Set sg equal to the samples that are covered by the rule
     sg = data[classifier.rule_list[i].covered_examples].copy()
     # Create a copy of Stability to be clustered by Ckmeans
     r_x = ro.FloatVector(sg['Stability'])
     # Call the clustering function for 2 clusters
     c1_x = ck(r_x, 2)
     # Retrieve the array of clusters generated by R
     c2 = np.array(ro.IntVector(c1_x[0]))
     # Create a Cluster2 feature in SG and save it.  
     sg['Cluster2'] = c2
     sg.to_csv("SG" + str(i) + "_Class_0.csv")
     
     print("SG" + str(i) +"_Class_0\n", sg.groupby('Class')['Class'].count())
     print("Cluster counts:", sg.groupby('Cluster2')['Cluster2'].count())
     # c is the instances of the sg that are in Cluster 1 (the low performance cluster)
     c = sg[sg.Cluster2 == 1]
     # Create a feature in the data based upon membership to the low performance sg cluster
     X.loc[c.index, 'Cluster_feature'] = 1
     # Create three sets using the dropped feature: 
     # One for those in the low performance sg cluster.  
     hf1 = dropped_feature.loc[c.index]

     # Get dropped features in the high performance sg cluster (cluster == 2)
     hf2 = dropped_feature.loc[sg[sg.Cluster2==2].index]
     # Get all of the dropped features not in the sg
     not_hf = dropped_feature.loc[~dropped_feature.index.isin(hf1.index)]
     # Run prediction, score, and plot.
     for train, test in kf.split(X, y):
        current_predictions[test] = clf.fit(X.iloc[train], y[train]).predict(X.iloc[test])
        probas_ = clf.predict_proba(X.iloc[test])
        current_probabilities[test] = probas_[:, 1]
     accuracy = np.mean(y == current_predictions)
     print("Score for SG" + str(i) +"_Class_0", accuracy)
     np.savetxt("SG" + str(i) + "_Class_0_predictions.csv", np.column_stack((current_predictions, y)), delimiter=",", fmt="%1d")
     fpr, tpr, _ = roc_curve(y, current_probabilities)
     plt.plot(fpr, tpr, label="ROC SG" + str(i) + "_Class_0 (Accuracy = %0.4f, AUC = %0.4f)" % (accuracy, roc_auc_score(y, current_probabilities)))
     print(X.groupby('Cluster_feature')['Cluster_feature'].count())
     # Perform t-test using inner sg dropped_feature and low performance sg cluster vs the remaining dropped features. 
     # The hidden feature is binomially distributed.  I'm using the fisher exact test to check for statistical significance.
     
     hf1_true = len(np.where(hf1==1)[0])
     hf1_false = len(np.where(hf1==0)[0])
     hf2_true = len(np.where(hf2==1)[0])
     hf2_false = len(np.where(hf2==0)[0])
     not_hf_true = len(np.where(not_hf==1)[0])
     not_hf_false = len(np.where(not_hf==0)[0])
     
     hf1_hf2_vector = ro.FloatVector([hf1_true, hf1_false, hf2_true, hf2_false])
     hf1_nothf_vector = ro.FloatVector([hf1_true, hf1_false, not_hf_true, not_hf_false])
     
     hf1_hf2_matrix = ro.r['matrix'](hf1_hf2_vector, nrow=2)
     hf1_nothf_matrix = ro.r['matrix'](hf1_nothf_vector, nrow=2)
     
     pvalue_sg = np.array(ro.FloatVector(fisher(hf1_hf2_matrix)[0]))
     pvalue_all = np.array(ro.FloatVector(fisher(hf1_nothf_matrix)[0]))

     # Write it all to the results dataframe.  
     results = results.append({'SG':'SG' + str(i) +'_Class_0', 'Rule' : str(classifier.rule_list[i]), 'SG_size': sg.shape[0], 'count_Class_0': len(sg[sg.Class==0]), 'count_Class_1': len(sg[sg.Class==1]), 'Accuracy':accuracy, 'AUROC':roc_auc_score(y, current_probabilities), 'len_hf1':len(hf1), 'len_hf2':len(hf2), 'len_nothf': len(not_hf), 'num_hf1_true':hf1_true, 'num_hf2_true':hf2_true, 'num_nothf_true':not_hf_true, 'hf1_pvalue_hf2': pvalue_sg[0],  'hf1_pvalue_nothf':pvalue_all[0]}, ignore_index = True)
     X['Cluster_feature'] = 0

# This loop finds the index of the first rule for Class == 1     
count = 0
for rule in classifier.rule_list:
    if rule.target_class == 1:
        break
    count = count + 1
    
current_predictions = np.empty(y.shape)
current_probabilities = np.empty(y.shape)

# This is a repeat of the above process, now for Class == 1
for i in range(0,3):
     
     # Set sg equal to the samples that are covered by the rule
     sg = data[classifier.rule_list[count + i].covered_examples].copy()
     # Create a copy of Stability to be clustered by Ckmeans
     r_x = ro.FloatVector(sg['Stability'])
     # Call the clustering function for 2 clusters
     c1_x = ck(r_x, 2)
     # Retrieve the array of clusters generated by R
     c2 = np.array(ro.IntVector(c1_x[0]))
     # Create a Cluster2 feature in SG and save it.  
     sg['Cluster2'] = c2
     sg.to_csv("SG" + str(i) + "_Class_1.csv")
     
     print("SG" + str(i) +"_Class_1\n", sg.groupby('Class')['Class'].count())
     print("Cluster counts:", sg.groupby('Cluster2')['Cluster2'].count())
     # c is the instances of the sg that are in Cluster 1 (the low performance cluster)
     c = sg[sg.Cluster2 == 1]
     # Create a feature in the data based upon membership to the low performance sg cluster
     X.loc[c.index, 'Cluster_feature'] = 1
     # Create three sets using the dropped feature: 
     # One for those in the low performance sg cluster.  
     hf1 = dropped_feature.loc[c.index]

     # Get dropped features in the high performance sg cluster (cluster == 2)
     hf2 = dropped_feature.loc[sg[sg.Cluster2==2].index]
     # Get all of the dropped features not in the sg
     not_hf = dropped_feature.loc[~dropped_feature.index.isin(hf1.index)]
     # Run prediction, score, and plot.
     for train, test in kf.split(X, y):
        current_predictions[test] = clf.fit(X.iloc[train], y[train]).predict(X.iloc[test])
        probas_ = clf.predict_proba(X.iloc[test])
        current_probabilities[test] = probas_[:, 1]
     accuracy = np.mean(y == current_predictions)
     print("Score for SG" + str(i) +"_Class_1", accuracy)
     np.savetxt("SG" + str(i) + "_Class_1_predictions.csv", np.column_stack((current_predictions, y)), delimiter=",", fmt="%1d")
     fpr = dict()
     tpr = dict()
     fpr, tpr, _ = roc_curve(y, current_probabilities )
     plt.plot(fpr, tpr, label="ROC SG" + str(i) + "_Class_1 (Accuracy = %0.4f, AUC = %0.4f)" % (accuracy, roc_auc_score(y, current_probabilities)))
     print(X.groupby('Cluster_feature')['Cluster_feature'].count())
     # Perform t-test using inner sg dropped_feature and low performance sg cluster vs the remaining dropped features.  
     hf1_true = len(np.where(hf1==1)[0])
     hf1_false = len(np.where(hf1==0)[0])
     hf2_true = len(np.where(hf2==1)[0])
     hf2_false = len(np.where(hf2==0)[0])
     not_hf_true = len(np.where(not_hf==1)[0])
     not_hf_false = len(np.where(not_hf==0)[0])
     
     hf1_hf2_vector = ro.FloatVector([hf1_true, hf1_false, hf2_true, hf2_false])
     hf1_nothf_vector = ro.FloatVector([hf1_true, hf1_false, not_hf_true, not_hf_false])
     
     hf1_hf2_matrix = ro.r['matrix'](hf1_hf2_vector, nrow=2)
     hf1_nothf_matrix = ro.r['matrix'](hf1_nothf_vector, nrow=2)
     
     pvalue_sg = np.array(ro.FloatVector(fisher(hf1_hf2_matrix)[0]))
     pvalue_all = np.array(ro.FloatVector(fisher(hf1_nothf_matrix)[0]))
     
     # Write it all to the results dataframe.  
     results = results.append({'SG':'SG' + str(i) +'_Class_1', 'Rule' : str(classifier.rule_list[count + i]), 'SG_size': sg.shape[0], 'count_Class_0': len(sg[sg.Class==0]), 'count_Class_1': len(sg[sg.Class==1]), 'Accuracy':accuracy, 'AUROC':roc_auc_score(y, current_probabilities), 'len_hf1':len(hf1), 'len_hf2':len(hf2), 'len_nothf': len(not_hf), 'num_hf1_true':hf1_true, 'num_hf2_true':hf2_true, 'num_nothf_true':not_hf_true, 'hf1_pvalue_hf2': pvalue_sg[0],  'hf1_pvalue_nothf':pvalue_all[0]}, ignore_index = True)
     X['Cluster_feature'] = 0
     
st = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d_%H%M')
results.to_csv('cad-sdtock-results-' + str(st) + '.csv', index=False)   
plt.legend(loc="lower right")
plt.title("ROC Curves for Subgroup Discovery to Ckmeans (CAD)")
plt.tight_layout()
plt.show()


#######################################################################################################
# Ckmeans to SD 
#######################################################################################################
os.chdir('D:\\TTU\\NEL_Research\\SD Work\\CAD\\CkmeansToSD')

learner = Orange.classification.CN2SDUnorderedLearner()

learner.rule_finder.search_algorithm.beam_width = 5
learner.weighted_cover_and_remove = 0.7
learner.rule_finder.general_validator.max_rule_length= 5

classifier = learner(Orange.data.Table('cktosd-cad-orange-format.csv'))



data = pd.read_csv('..\\UQ_rf_clusters_results.csv')
y = data.Class.ravel()
X = data.drop(['Class', 'Stability', 'Cluster2'], axis=1)

X['Cluster_feature'] = 0

columns = ['SG','Rule', 'SG_size', 'count_Class_0', 'count_Class_1', 'Accuracy', 'AUROC', 'len_hf1', 'len_nothf', 'num_hf1_true', 'num_nothf_true', 'hf1_pvalue_nothf']
results = pd.DataFrame(columns = columns)
clf = RandomForestClassifier(n_jobs=-1)
predictions = np.empty(shape=(X.shape[0], 5))
probabilities = np.empty(shape=(len(y), 5))
indeces = []
# Take the top five rules for the low performing cluster (cluster 1)
plt.figure('cktosd', figsize=(7,7))
for j in range(0,5):
    # Get the sg based upon examples covered by the rules.  
    sg = data[classifier.rule_list[j].covered_examples].copy()
    print("Rule distributions for rule " + str(j) + ":", classifier.rule_list[j].curr_class_dist)
    sg.to_csv("SG" + str(j) + ".csv")
    print("SG" + str(j) +" actual distro count \n",sg.groupby('Cluster2')['Cluster2'].count())
    # Create a feature based upon membership to the sg
    #X.loc[sg.index, 'Cluster_feature'] = 1
    ## This is yielding crap results when I include every sample.  How about I just include the ones ACTUALLY in cluster 1
    X.loc[sg[sg.Cluster2 == 1].index, 'Cluster_feature'] = 1
    indeces.append(sg.index)
    # Get dropped features for those in the sg and those not in the sg.
    hf1 = dropped_feature.loc[sg.index]
    not_hf = dropped_feature.loc[~dropped_feature.index.isin(hf1.index)]    
    
    for train, test in kf.split(X, y):
        clf.fit(X.iloc[train], y[train])
        current_predictions[test] = clf.predict(X.iloc[test])
        probas_ = clf.predict_proba(X.iloc[test])
        current_probabilities[test] = probas_[:, 1]
    predictions[:, j:j+1] = np.vstack(current_predictions)   
    probabilities[:, j:j+1] = np.vstack(current_probabilities)
    accuracy = np.mean(y == current_predictions)
    print("Score SG" + str(j) + "", accuracy)
    np.savetxt("SG" + str(j) + "_predictions.csv", np.column_stack((current_predictions, y)), delimiter=",", fmt="%1d")
    fpr, tpr, _ = roc_curve(y, current_probabilities)
    plt.plot(fpr, tpr, label="ROC SG" + str(j) + " (Accuracy = %0.4f, AUC = %0.4f)" % (accuracy, roc_auc_score(y, current_probabilities)))
    print(X.groupby('Cluster_feature')['Cluster_feature'].count())
    
    hf1_true = len(np.where(hf1==1)[0])
    hf1_false = len(np.where(hf1==0)[0])

    not_hf_true = len(np.where(not_hf==1)[0])
    not_hf_false = len(np.where(not_hf==0)[0])
     
    hf1_nothf_vector = ro.FloatVector([hf1_true, hf1_false, not_hf_true, not_hf_false])
     
    hf1_nothf_matrix = ro.r['matrix'](hf1_nothf_vector, nrow=2)
    
    pvalue_all = np.array(ro.FloatVector(fisher(hf1_nothf_matrix)[0]))
    
    results = results.append({'SG':'SG' + str(j),'Rule': str(classifier.rule_list[j]), 'SG_size': sg.shape[0], 'count_Class_0': len(sg[sg.Class==0]), 'count_Class_1': len(sg[sg.Class==1]), 'Accuracy':accuracy, 'AUROC':roc_auc_score(y, current_probabilities), 'len_hf1':len(hf1), 'len_nothf': len(not_hf), 'num_hf1_true':hf1_true, 'num_nothf_true':not_hf_true, 'hf1_pvalue_nothf':pvalue_all[0]}, ignore_index = True)    
    X['Cluster_feature'] = 0
st = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d_%H%M')
results.to_csv('cad-cktosd-results-' + str(st) + '.csv', index=False)
plt.legend(loc="lower right")
plt.title("ROC Curves for Ckmeans to Subgroup Discovery (CAD)")
plt.tight_layout()
plt.show()